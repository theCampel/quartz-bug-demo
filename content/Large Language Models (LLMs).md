> [!hint] Cool Thing to Think About ðŸ¤”
> It's not immediately obvious, but LLMs are (some of?) the first [[AI|Machine Learning]] models that you can improve the accuracy ***without*** changing the [[weights]]... You just have to include few-shot examples in the context. 


### Where do I start?
[[Natural Language Processing|Natural Language]] is an incredibly tough challenge to give an AI. It's one of humanity's oldest [[Technology|technologies]]. Thanks to being bilingual, it's clear to me that it's a tool for exchanging *abstract concepts* from one brain to another. But at the root of it, we don't think in English or Spanish. We think in concepts. 

> [!summary] What's an LLM? (*extremely painfully simplified*)
> *It's an algorithm that reads **a lot** of sentences and learns the **abstract representation** of their **concepts**. Based on this, it then predicts the next word in a sentence*. Can also be thought of as a lossy compression algorithm. 

Language models are not new (think autocomplete on your phone). But thanks to a group of researchers from Google in 2017, they've supercharged recently. They realised a key technique [[Attention (AI)|Attention]] was all you needed ðŸ˜‰. Most LLMs today (September 2023) are powered by some variation of the extremely powerful (Google-discovered) [[Transformer]]. 

